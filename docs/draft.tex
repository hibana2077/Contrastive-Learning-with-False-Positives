\begin{document}

\maketitle

\begin{abstract}%
We study long-form video compression under a global token budget, where each time step contains many spatial tokens and only a subset can be queried.
Under a sparse-event generative model with slowly drifting background, we analyze a simple \emph{time-first screening, space-second refinement} scheme.
Our main results provide: (i) high-probability event screening guarantees from inexpensive per-time sampling, (ii) end-to-end accuracy bounds for preserving a fixed linear downstream score, including an explicit deterministic bias term induced by background drift anchoring, and (iii) an information-theoretic lower bound showing an $\Omega\!\big(\tfrac{\sigma^2}{\Delta^2}s\log(T/s)\big)$ barrier for localizing rare events.
We also clarify the practical meaning of the separability condition $\Delta>2\gamma$: the \emph{margin} $\kappa:=\Delta-2\gamma$ governs stability and sample complexity, and guarantees degrade sharply as $\kappa\downarrow 0$.
\end{abstract}

\begin{keywords}%
Long-form video; token budget; time--space decoupling; sparse events; change detection; subGaussian noise; information-theoretic lower bounds%
\end{keywords}

\section{Introduction}
\section{Setup}
\label{sec:setup}

% NOTE: The current draft already includes substantial setup material inside
% \S\ref{sec:main_results}. If you paste this section in, consider moving (not duplicating)
% the overlapping definitions to keep the paper concise.

\paragraph{Query model and token budget.}
A long-form video is indexed by time $t\in[T]:=\{1,\dots,T\}$.
At each time step $t$, we observe a set of $N$ spatial tokens
$\{x_{t,i}\in\mathbb{R}^d\}_{i=1}^N$.
A \emph{query} reveals the full vector $x_{t,i}$ and costs one unit from a global token budget $B$.
When we sample multiple tokens within the same time step, we assume \emph{uniform sampling without replacement}
from $\{1,\dots,N\}$.

\paragraph{Task: preserve a fixed linear score.}
We analyze a task-aligned surrogate that is simple enough to admit clean guarantees, but still captures
long-video recognition/retrieval heads after projection.
Let $w\in\mathbb{R}^d$ be a fixed (known) linear head with $\|w\|_2\le 1$.
Define the scalar projection
$$
y_{t,i}:=\langle w,x_{t,i}\rangle,
\qquad
\mu_t := \frac{1}{N}\sum_{i=1}^N x_{t,i},
\qquad
a_t := \langle w,\mu_t\rangle .
$$
The downstream quantity to preserve is the global average score
\begin{equation}
\label{eq:target_score}
s^\star
\;:=\;
\frac{1}{T}\sum_{t=1}^T a_t
\;=\;
\frac{1}{T}\sum_{t=1}^T \langle w,\mu_t\rangle .
\end{equation}
Our goal is to output an estimator $\hat s$ with small error $|\hat s-s^\star|$ using at most $B$ queries.

\paragraph{Sparse events with slowly drifting background.}
We assume an unknown event set $E\subseteq[T]$ that is a union of $s$ disjoint intervals,
$$
E \;=\; \bigcup_{j=1}^s [a_j,b_j],
\qquad
1\le a_j\le b_j\le T,
$$
and a generative decomposition
\begin{equation}
\label{eq:model}
x_{t,i}
\;=\;
m_t
\;+\;
\mathbf{1}\{t\in E\}\,\delta_t
\;+\;
\xi_{t,i},
\qquad
\mathbb{E}[\xi_{t,i}\mid t]=0 .
\end{equation}
We impose assumptions only in the \emph{task-relevant score direction} induced by $w$:
\begin{itemize}
\item \textbf{(SubGaussian noise in the score direction).}
$\langle w,\xi_{t,i}\rangle$ is $\sigma^2$-subGaussian.
\item \textbf{(Bounded background drift).}
For all $t\ge 2$,
\begin{equation}
\label{eq:drift}
\big|\langle w,m_t-m_{t-1}\rangle\big| \;\le\; \gamma .
\end{equation}
\item \textbf{(Event strength in the score direction).}
Let $\beta_t:=\langle w,\delta_t\rangle$.
Within each event interval $[a_j,b_j]$, $\beta_t$ is constant (denote it by $\beta_j$) and
\begin{equation}
\label{eq:signal}
|\beta_j| \;\ge\; \Delta .
\end{equation}
\end{itemize}
A key separability quantity is the \emph{screening margin}
$$
\kappa \;:=\; \Delta-2\gamma,
$$
which captures how well event-boundary changes separate from background drift after accounting for estimation noise.

\paragraph{Time--space decoupled compression scheme.}
We study a simple two-stage procedure that first spends a small, uniform budget to \emph{screen in time}
and then concentrates queries to \emph{refine in space}.

\smallskip
\noindent
\textbf{Stage I (temporal screening).}
For each $t\in[T]$, query $r$ tokens and form the projected sample mean
$$
\hat a_t
\;:=\;
\frac{1}{r}\sum_{i\in\mathcal{I}_t} y_{t,i},
\qquad
\mathcal{I}_t\subseteq[N],\ |\mathcal{I}_t|=r.
$$
Define the change statistic $g_t:=|\hat a_t-\hat a_{t-1}|$ (for $t\ge 2$), and declare candidate boundaries when
$g_t>\tau$ for a threshold $\tau$.
By merging adjacent detections, we obtain a partition $\widehat{\mathcal{P}}$ of $[T]$ into segments and
a set of \emph{event anchors} $\widehat T$ (e.g., one representative index per detected event segment).

\smallskip
\noindent
\textbf{Stage II (spatial refinement and background anchoring).}
For each event anchor $t\in\widehat T$, query $m$ additional tokens and form a refined estimate $\tilde a_t$.
To control drift within long background segments, we also insert \emph{background anchors} every $L$ time steps,
forming a set $\widehat B$ with $|\widehat B|=\Theta(T/L)$; at each $b\in\widehat B$ we query $m_0$ tokens and
form $\tilde a_b$.
Each segment $S\in\widehat{\mathcal{P}}$ is assigned a background anchor $b(S)\in \widehat B\cap S$
(e.g., the midpoint anchor under the length cap $L$), and we estimate the per-time means in $S$
by the corresponding anchor value.
The final estimator averages refined anchors:
\begin{equation}
\label{eq:estimator}
\hat s
\;:=\;
\frac{1}{T}
\Bigg(
\sum_{t\in \widehat T}\tilde a_t
\;+\;
\sum_{\text{segments }S\in\widehat{\mathcal{P}}}
\sum_{t\in S\setminus\widehat T}
\tilde a_{b(S)}
\Bigg).
\end{equation}

\paragraph{Budget accounting and the $L$ tradeoff.}
The total number of queried tokens is
\begin{equation}
\label{eq:budget}
B
\;=\;
rT \;+\; m|\widehat T| \;+\; m_0|\widehat B|.
\end{equation}
The anchoring interval $L$ trades off (i) a deterministic approximation error from representing a drifting background
with finitely many anchors (scaling as $O(\gamma L)$ under~\eqref{eq:drift}) and (ii) the anchor overhead
$|\widehat B|=\Theta(T/L)$ in~\eqref{eq:budget}.

\section{Main Results}
\label{sec:main_results}

This section establishes a provable baseline for \emph{time--space decoupled} compression of long-form videos under a global token budget.
The key message is that, when task-relevant temporal structure is sparse, one can (i) spend a small, uniform budget to \emph{screen in time} for statistically significant changes, and then (ii) spend a larger, concentrated budget to \emph{refine in space} only on a small set of segment anchors.
We also highlight an information-theoretic barrier for temporal localization.

Throughout, we analyze preservation of a fixed downstream \emph{linear scoring head}.

\paragraph{Token model.}
The video consists of $T$ time steps.
At each time $t$, there are $N$ spatial tokens $\{x_{t,i}\in\mathbb{R}^d\}_{i=1}^N$.
Querying a token reveals its feature vector and incurs unit cost toward a global budget $B$.
Within each time step, tokens are sampled \emph{uniformly without replacement}.

\paragraph{Downstream objective and task-relevant projection.}
Let $w\in\mathbb{R}^d$ be a \emph{fixed and known} linear head with $\|w\|_2\le1$.
Define the scalar token
$
y_{t,i}:=\langle w,x_{t,i}\rangle
$
and the (unknown) time-step mean
$
a_t := \langle w,\mu_t\rangle
$
where $\mu_t := \frac{1}{N}\sum_{i=1}^N x_{t,i}$.
The target score is
\begin{equation}
\label{eq:target_score}
s^\star
\;:=\;
\frac1T \sum_{t=1}^T a_t
\;=\;
\frac1T \sum_{t=1}^T \langle w,\mu_t\rangle .
\end{equation}
Our goal is to output an estimate $\hat s$ with $|\hat s-s^\star|$ small, using at most $B$ token queries.

\paragraph{Sparse events with drifting background (projected model).}
We assume that the score-direction process $a_t$ admits a sparse event + slowly drifting background decomposition.
Specifically, there exists an unknown event set
$$
E=\bigcup_{j=1}^s [a_j,b_j]\subseteq\{1,\dots,T\}
$$
given by a union of $s$ disjoint intervals, and a decomposition
\begin{equation}
\label{eq:model}
x_{t,i} = m_t + \mathbf{1}\{t\in E\}\,\delta_t + \xi_{t,i},
\qquad
\mathbb{E}[\xi_{t,i}\mid t]=0,
\end{equation}
where the noise is $\sigma^2$-subGaussian in the score direction:
$\langle w,\xi_{t,i}\rangle$ is $\sigma^2$-subGaussian.

We impose two task-aligned conditions on the \emph{projected} mean sequence $\{a_t\}_{t=1}^T$:
\begin{itemize}
\item \textbf{(Bounded drift in the score direction).}
For all $t\ge2$,
\begin{equation}
\label{eq:drift}
| \langle w,m_t-m_{t-1}\rangle | \;\le\; \gamma .
\end{equation}

\item \textbf{(Piecewise-constant event shift in the score direction).}
Let $\beta_t := \langle w,\delta_t\rangle$.
We assume $\beta_t$ is constant within each event interval and bounded away from $0$:
for each $j\in[s]$, there exists $\beta_j$ such that $\beta_t=\beta_j$ for all $t\in[a_j,b_j]$, and
\begin{equation}
\label{eq:signal}
|\beta_j| \;\ge\; \Delta .
\end{equation}
Equivalently, $\{a_t\}$ is a slowly drifting background plus sparse mean-level shifts.
This condition rules out arbitrarily many within-event oscillations in the \emph{task-relevant} direction, which is necessary to obtain an $O(s+\log T)$ bound on the number of detected change points.
(If $\beta_t$ has $J$ additional jumps, all bounds below hold with $s$ replaced by $s+J$.)
\end{itemize}

\paragraph{Two-stage decoupled scheme (screen in time, refine in space).}
\textbf{Stage I (temporal screening).}
At each time $t$, we query $r$ tokens and form the sample mean
$
\hat a_t := \frac{1}{r}\sum_{i\in \mathcal{I}_t} y_{t,i}
$
where $\mathcal{I}_t$ is a size-$r$ subset sampled uniformly without replacement from $\{1,\dots,N\}$.
We define the screening statistic $g_t := |\hat a_t-\hat a_{t-1}|$.
Times with $g_t>\tau$ are marked as candidate boundaries; consecutive detections are merged to form detected boundary blocks, from which we construct a partition $\widehat{\mathcal{P}}$ and a set of segment anchors $\widehat T$.

\textbf{Stage II (spatial refinement + background anchoring).}
For each detected event anchor $t\in \widehat T$, we query $m$ additional tokens at time $t$ and form a refined estimate $\tilde a_t$.
For each background segment, we also insert an anchor every $L$ time steps (a length cap), producing a set of background anchors $\widehat B$ with $|\widehat B|=\Theta(T/L)$; at each $b\in\widehat B$ we query $m_0$ tokens and form $\tilde a_b$.
The final estimator averages refined anchors:
\begin{equation}
\label{eq:estimator}
\hat s
\;:=\;
\frac1T
\Bigg(
\sum_{t\in \widehat T}\tilde a_t
\;+\;
\sum_{\text{segments }S\in\widehat{\mathcal{P}}}
\sum_{t\in S\setminus\widehat T}
\tilde a_{b(S)}
\Bigg),
\end{equation}
where $b(S)\in \widehat B$ denotes the background anchor assigned to segment $S$ (e.g., the midpoint anchor under the length cap).

\begin{table}[t]
\centering
\small
\begin{tabular}{l p{4.2cm} p{5.3cm}}
\hline
\textbf{Result} & \textbf{Guarantee (output)} & \textbf{Key conditions and token scaling} \\
\hline
Thm.~\ref{thm:screening} & Detect all true entry/exit boundaries (up to small localization error) and bound $|\widehat{\partial E}|$ & Margin $\kappa=\Delta-2\gamma>0$; $r=\Theta\!\big(\sigma^2\kappa^{-2}\log(T/\eta)\big)$; suitable threshold $\tau$ \\
Thm.~\ref{thm:accuracy} & End-to-end score accuracy $|\hat s-s^\star|\le \varepsilon + O(\gamma L)$ & Refinement $m,m_0=\Theta\!\big(\sigma^2\varepsilon^{-2}\log(\cdot/\eta)\big)$; anchor overhead $|\widehat B|=\Theta(T/L)$ \\
Thm.~\ref{thm:lower} & Localization requires $\Omega\!\big(\frac{\sigma^2}{\Delta^2}s\log(T/s)\big)$ queries & Holds for adaptive querying; dimension-robust; applies to our drift model by restriction to $m_t\equiv 0$ \\
\hline
\end{tabular}
\caption{Summary of the main results (read each row left-to-right: the guarantee and the corresponding key conditions / token scaling). Here $r$ is the per-time screening sample size, $m$ and $m_0$ are the per-anchor refinement sample sizes, and $L$ is the background anchoring interval controlling the drift-bias/overhead tradeoff.}
\label{tab:main_results_summary}
\end{table}

\paragraph{Parameter roles (one-line intuition).}
All quantities are defined in the \emph{task-relevant score direction} induced by $w$.
The event magnitude $\Delta$ lower bounds the jump size $|\beta_j|$ during each event interval, while $\gamma$ upper bounds the per-step background drift in~\eqref{eq:drift}.
Their difference $\kappa:=\Delta-2\gamma$ is the screening \emph{margin}: it is the separation between typical background changes (size $\lesssim\gamma$) and event boundary changes (size $\gtrsim\Delta-\gamma$), after accounting for estimation noise.
Finally, $L$ is the background anchoring interval length (maximum segment length before inserting a new background anchor), trading deterministic drift bias $O(\gamma L)$ against anchor overhead $\Theta(T/L)$ in~\eqref{eq:budget}.

\paragraph{Guarantees overview.}
Our results answer three questions:
(i) When does inexpensive temporal screening identify all true event boundaries (up to small localization error)?
(ii) Given a (nearly correct) segmentation, how many additional spatial tokens suffice for end-to-end score accuracy, and what error terms remain?
(iii) What is the irreducible token cost of temporal localization under sparse events?

\begin{theorem}[Projected boundary screening under sparse events]
\label{thm:screening}
Assume $\Delta>2\gamma$ and define $\kappa:=\Delta-2\gamma$.
There exists a universal constant $C>0$ such that if
\begin{equation}
\label{eq:r_condition}
r \;\ge\;
C\,\frac{\sigma^2}{\kappa^2}\log\frac{T}{\eta},
\end{equation}
and $\tau$ is chosen in an admissible interval of width $\Theta(\kappa)$ (e.g., $\tau=\Delta/2$), then with probability at least $1-\eta$:
\begin{enumerate}
\item every true event boundary (entry or exit) is detected by $\widehat{\partial E}$ up to a $\pm 1$ time-step localization error;
\item the number of detected boundaries satisfies
$
|\widehat{\partial E}| = O\!\big(s+\log(T/\eta)\big),
$
and therefore the number of segments in $\widehat{\mathcal{P}}$ is $O\!\big(s+\log(T/\eta)\big)$.
\end{enumerate}
\end{theorem}

\paragraph{On the role of the $\pm 1$ localization statement.}
The $\pm 1$ guarantee in Theorem~\ref{thm:screening} corresponds to the simplest pointwise statistic $g_t=|\hat a_t-\hat a_{t-1}|$.
More generally, if one replaces $g_t$ by a windowed statistic (e.g., comparing averages over windows of size $h$), the same argument yields localization up to $\pm O(h)$ while typically improving robustness to noise (at the expense of constants).
Crucially, our Stage~II analysis does \emph{not} rely on exact boundary indices: small localization errors only perturb a constant number of segment endpoints, and the estimator~\eqref{eq:estimator} remains valid after the enforced segment-length cap $L$.

\paragraph{Sampling without replacement.}
The concentration needed for Theorem~\ref{thm:screening} is for the sample mean of $\{y_{t,i}\}_{i=1}^N$ under \emph{uniform sampling without replacement}.
A Serfling/Bernstein--Serfling inequality yields the same $r=\Theta(\sigma^2\kappa^{-2}\log(T/\eta))$ scaling as in the i.i.d.\ case, up to the standard finite-population correction factor.

\paragraph{Why the margin $\kappa$ behaves like a hard threshold.}
The screening statistic compares adjacent projected means.
Under the model, non-boundary times satisfy $|a_t-a_{t-1}|\le\gamma$, whereas true boundaries satisfy $|a_t-a_{t-1}|\ge\Delta-\gamma$.
Hence the admissible threshold interval that separates the two regimes has width $\Theta(\kappa)$.
As $\kappa\downarrow 0$, this interval collapses and the required screening sample size diverges as $r=\Theta(\sigma^2\kappa^{-2}\log(T/\eta))$ in~\eqref{eq:r_condition}.
If $r$ is not increased accordingly, one observes a phase transition to one of two failure modes: either (i) many false positives (up to $\Theta(T)$) when the threshold is set near the background scale, or (ii) systematic miss-detections when the threshold is set near the event scale.
When $\kappa\le 0$, background drift alone can produce event-sized changes in the score direction, so no threshold can simultaneously control both error types.

\paragraph{Threshold choice: failure modes and a simple practical rule.}
Theorem~\ref{thm:screening} is stated with an admissible interval parameterized by $(\Delta,\gamma)$; this interval has width $\Theta(\kappa)$ and disappears as $\kappa\downarrow0$.
It is useful to make the two canonical failure modes explicit:
\begin{itemize}
\item If $\tau$ is \emph{too small} (below the effective background scale), then drift and noise trigger many detections and false positives can grow to $\Theta(T)$ in the worst case, destroying the $O(s+\log T)$ segmentation size guarantee.
\item If $\tau$ is \emph{too large} (above the event-induced jump scale), then true boundaries fall below threshold and screening suffers miss-detections.
\end{itemize}
In practice $(\Delta,\gamma)$ are unknown.
A simple data-driven choice that can be stated in the main text is:
(i) compute $g_t=|\hat a_t-\hat a_{t-1}|$ from Stage~I samples;
(ii) let $\tilde g:=\mathrm{median}(\{g_t\}_{t=2}^T)$ and $\mathrm{MAD}:=\mathrm{median}(|g_t-\tilde g|)$;
(iii) set
\begin{equation}
\label{eq:tau_practical}
\hat\tau
\;:=\; \tilde g
\;+
c\,\mathrm{MAD}\,\sqrt{\log(T/\eta)}
\end{equation}
for a moderate constant $c$.
Under event sparsity (so most $g_t$ are generated by background) and margin $\kappa>0$, this places $\hat\tau$ above the background tail while remaining below the event jump scale, i.e., inside an admissible $\Theta(\kappa)$ interval with high probability.
We formalize one such calibration guarantee in Appendix~\ref{app:unknown_delta}.

\begin{theorem}[End-to-end score accuracy and drift-induced bias]
\label{thm:accuracy}
Assume $\|w\|_2\le1$ and $\Delta>2\gamma$.
If
\begin{equation}
\label{eq:m_conditions}
m \;\ge\;
C\,\frac{\sigma^2}{\varepsilon^2}\log\frac{|\widehat T|}{\eta},
\qquad
m_0 \;\ge\;
C\,\frac{\sigma^2}{\varepsilon^2}\log\frac{|\widehat B|}{\eta},
\end{equation}
then with probability at least $1-\eta$,
\begin{equation}
\label{eq:accuracy_bound}
|\hat s - s^\star|
\;\le\;
\varepsilon \;+\;
\underbrace{O(\gamma L)}_{\text{deterministic drift bias}}.
\end{equation}
Moreover, the total token usage satisfies
\begin{equation}
\label{eq:budget}
B
\;=\;
rT \;+\; m|\widehat T| \;+\; m_0|\widehat B|
\;=\;
\tilde O\!\left(
T\frac{\sigma^2}{(\Delta-2\gamma)^2}
\;+\;
(s+\log T)\frac{\sigma^2}{\varepsilon^2}
\;+\;
\frac{T}{L}\frac{\sigma^2}{\varepsilon^2}
\right).
\end{equation}
\end{theorem}

\paragraph{Interpreting the $O(\gamma L)$ term (constants and tightness).}
Theorem~\ref{thm:accuracy} separates (i) a statistical term $\varepsilon$, which can be reduced by increasing $(m,m_0)$, from (ii) a structural term induced by approximating a drifting mean process with finitely many anchors.
Concretely, for any segment $S$ and anchor $b(S)\in S$,
\begin{equation}
\label{eq:bias_radius}
\left|\frac{1}{|S|}\sum_{t\in S} a_t - a_{b(S)}\right|
\;\le\;
\gamma\cdot \max_{t\in S}|t-b(S)|.
\end{equation}
Thus, if $|S|\le L$ and $b(S)$ is the midpoint, then $\max_{t\in S}|t-b(S)|\le L/2$ and the drift bias in~\eqref{eq:accuracy_bound} can be taken as at most $(\gamma L)/2$ (up to rounding).
If $b(S)$ is chosen arbitrarily (or randomly), the worst-case radius can be as large as $L$, recovering the stated $O(\gamma L)$ form.
This dependence on $\gamma L$ is also worst-case tight: for a monotone drift that changes by $\gamma$ per step, any single-anchor representation of a length-$L$ segment incurs $\Omega(\gamma L)$ average approximation error.

\paragraph{When can we take $m=m_0$ (without loss)?}
The split between $(m_0,|\widehat B|)$ and $(m,|\widehat T|)$ is used only to highlight that background anchoring and event refinement contribute additively to the budget.
In many regimes, setting $m_0\asymp m$ is natural, and then~\eqref{eq:budget} makes explicit how the remaining free parameter $L$ controls the anchor overhead.

\paragraph{How to choose the anchoring interval $L$ (order-wise).}
Theorem~\ref{thm:accuracy} makes the $L$-dependence explicit via two competing terms: the deterministic drift bias $O(\gamma L)$ and the anchor overhead $|\widehat B|=\Theta(T/L)$ in~\eqref{eq:budget}.
A systematic way to pick $L$ given a target total error level $\epsilon_{\mathrm{tot}}$ is to budget half the error to statistics and half to drift:
set $\varepsilon:=\epsilon_{\mathrm{tot}}/2$ in~\eqref{eq:m_conditions} and choose
\begin{equation}
\label{eq:L_choice}
L
\;=\; \Theta\!\Big(\min\{T,\,\epsilon_{\mathrm{tot}}/\gamma\}\Big)
\end{equation}
so that $O(\gamma L)=O(\epsilon_{\mathrm{tot}})$.
This choice is also budget-optimal up to constants among all $L$ that satisfy the same drift-bias constraint, since the anchoring term $T/L$ decreases with $L$.

\noindent
\textbf{Applicability.} In the slow-drift regime $\gamma\ll \epsilon_{\mathrm{tot}}$,~\eqref{eq:L_choice} yields $L\gg1$ and hence $T/L\ll T$, so time--space decoupling can strictly reduce token usage relative to uniform refinement.
In the moderate-drift regime $\gamma\approx \epsilon_{\mathrm{tot}}$, the optimal choice is $L=\Theta(1)$ and the anchoring overhead becomes $\Theta(T)$, so the practical advantage may diminish.
If $\gamma\gg \epsilon_{\mathrm{tot}}$, no choice of $L\ge1$ can keep the drift bias below $\epsilon_{\mathrm{tot}}$ using a single-anchor-per-block representation; one then needs either a richer within-block model (more anchors) or a different structural assumption on the background.

\subsection{Information-theoretic lower bound}

\begin{theorem}[Necessity of temporal identification (localization lower bound)]
\label{thm:lower}
Consider a simplified model with constant background ($m_t\equiv0$), $s$ isolated event times, and a fixed event shift vector $\delta$ with $\|\delta\|_2=\Delta$.
For any (possibly adaptive) algorithm that queries fewer than
\begin{equation}
\label{eq:lower_bound}
B
\;<\;
c\,\frac{\sigma^2}{\Delta^2}
\,s\log\frac{T}{s}
\end{equation}
tokens, there exists a distribution under which the algorithm fails with probability at least $1/3$ to correctly identify the event set.
\end{theorem}

\paragraph{Compatibility with the drift model.}
Although Theorem~\ref{thm:lower} is stated for $m_t\equiv 0$, it applies \emph{a fortiori} to the drift model in~\eqref{eq:model}--\eqref{eq:drift}: any algorithm designed to handle drifting backgrounds must also succeed on the special case of zero drift.
More broadly, allowing drift can only make localization harder, so~\eqref{eq:lower_bound} remains a valid necessary condition.

\paragraph{Alignment with the query model and the role of $d$.}
Theorem~\ref{thm:lower} applies to fully adaptive token queries: at each step, the algorithm may choose which $(t,i)$ to query based on all past observations.
The lower bound is dimension-robust because it can be witnessed by a one-dimensional construction: it suffices to consider instances where all signal and noise lie in the span of $\delta$ (equivalently, project observations onto $\delta/\|\delta\|_2$).
Thus, allowing $d>1$ and revealing full vectors does not circumvent the $s\log(T/s)$ localization barrier.

\paragraph{Tightness and remaining gaps.}
Theorem~\ref{thm:lower} implies that any procedure that must localize $s$ rare event times among $T$ candidates incurs an information-theoretic cost of order
$\Omega\!\big(\tfrac{\sigma^2}{\Delta^2}s\log(T/s)\big)$ token queries (up to constants).
Our Stage~I analysis provides a clean, single-pass sufficient condition via a scan over all $T$ times (cost $rT$ in~\eqref{eq:budget}).
Closing the gap between $rT$ and the $s\log(T/s)$ lower bound would require more adaptive or multi-scale localization strategies that avoid inspecting every time step.

\paragraph{What this lower bound does (and does not) imply.}
Theorem~\ref{thm:lower} is a lower bound for \emph{localization} (identifying the event set), not directly for estimating the global score $s^\star$.
Therefore it does \emph{not} rule out the possibility of estimating $s^\star$ accurately without ever producing an explicit event set.
Our upper bounds analyze a concrete time-first screening scheme that does localize boundaries because this is the natural mechanism by which sparse events are preserved under a tight budget; alternative ``non-localizing'' estimators are an interesting direction but are not covered by our analysis.

\paragraph{Toward matching the localization lower bound.}
Promising directions include multiscale screening and group-testing style zoom-in procedures: e.g., start with a coarse subsampling of time, then iteratively allocate additional $r$-budget only to intervals whose projected statistics exceed a background-calibrated threshold.
Such schemes aim to concentrate queries near the (unknown) $O(s)$ boundary locations and could plausibly reduce the scan cost toward $\tilde O(s\log(T/s))$ while retaining the task-aligned projection that makes Theorem~\ref{thm:screening} dimension-robust.

\section{Conclusion}

% Acknowledgments---Will not appear in anonymized version
\acks{We thank a bunch of people and funding agency.}

\bibliography{ref}

\appendix

% \crefalias{section}{appendix} % uncomment if you are using cleveref

\section{app 1}
\label{app:unknown_delta}
\section{app 2}

\end{document}
